{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear regression workbook\n",
    "\n",
    "This workbook will walk you through a linear regression example. It will provide familiarity with Jupyter Notebook and Python.  Please print (to pdf) a completed version of this workbook for submission with HW #1.\n",
    "\n",
    "ECE C147/C247, Winter Quarter 2023, Prof. J.C. Kao, TAs: T.M, P.L, R.G, K.K, N.V, S.R, S.P, M.E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#allows matlab plots to be generated in line\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data generation\n",
    "\n",
    "For any example, we first have to generate some appropriate data to use. The following cell generates data according to the model: $y = x - 2x^2 + x^3 + \\epsilon$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)  # Sets the random seed.\n",
    "num_train = 200     # Number of training data points\n",
    "\n",
    "# Generate the training data\n",
    "x = np.random.uniform(low=0, high=1, size=(num_train,))\n",
    "y = x - 2*x**2 + x**3 + np.random.normal(loc=0, scale=0.03, size=(num_train,))\n",
    "f = plt.figure()\n",
    "ax = f.gca()\n",
    "ax.plot(x, y, '.')\n",
    "ax.set_xlabel('$x$')\n",
    "ax.set_ylabel('$y$')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QUESTIONS:\n",
    "\n",
    "Write your answers in the markdown cell below this one:\n",
    "\n",
    "(1) What is the generating distribution of $x$?\n",
    "\n",
    "(2) What is the distribution of the additive noise $\\epsilon$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ANSWERS:\n",
    "\n",
    "(1) $x$ is in uniform distribution which ranges from 0 to 1\n",
    "\n",
    "(2) $\\epsilon$ is in normal distribution which is symmetric about y-axis and the standard deviation is 0.03"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting data to the model (5 points)\n",
    "\n",
    "Here, we'll do linear regression to fit the parameters of a model $y = ax + b$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xhat = (x, 1)\n",
    "xhat = np.vstack((x, np.ones_like(x)))\n",
    "\n",
    "# ==================== #\n",
    "# START YOUR CODE HERE #\n",
    "# ==================== #\n",
    "# GOAL: create a variable theta; theta is a numpy array whose elements are [a, b]\n",
    "\n",
    "theta = np.zeros(2) # please modify this line\n",
    "xhat_T = xhat.T\n",
    "x_inv = np.linalg.pinv(xhat.dot(xhat_T))\n",
    "theta = x_inv.dot(xhat).dot(y)\n",
    "# ================== #\n",
    "# END YOUR CODE HERE #\n",
    "# ================== #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the data and your model fit.\n",
    "f = plt.figure()\n",
    "ax = f.gca()\n",
    "ax.plot(x, y, '.')\n",
    "ax.set_xlabel('$x$')\n",
    "ax.set_ylabel('$y$')\n",
    "\n",
    "# Plot the regression line\n",
    "xs = np.linspace(min(x), max(x),50)\n",
    "xs = np.vstack((xs, np.ones_like(xs)))\n",
    "plt.plot(xs[0,:], theta.dot(xs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QUESTIONS\n",
    "\n",
    "(1) Does the linear model under- or overfit the data?\n",
    "\n",
    "(2) How to change the model to improve the fitting?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ANSWERS\n",
    "\n",
    "(1) This linear model underfit the data because the line doesn't fit many datapoints of the training set.\n",
    "\n",
    "(2) We can use a polynomial model of a higher order to fit the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting data to the model (5 points)\n",
    "\n",
    "Here, we'll now do regression to polynomial models of orders 1 to 5.  Note, the order 1 model is the linear model you prior fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== #\n",
    "# START YOUR CODE HERE #\n",
    "# ==================== #\n",
    "\n",
    "# GOAL: create a variable thetas.\n",
    "# thetas is a list, where theta[i] are the model parameters for the polynomial fit of order i+1.\n",
    "#   i.e., thetas[0] is equivalent to theta above.\n",
    "#   i.e., thetas[1] should be a length 3 np.array with the coefficients of the x^2, x, and 1 respectively.\n",
    "#   ... etc.\n",
    "\n",
    "N = 5\n",
    "x2 = [i**2 for i in x]\n",
    "x3 = [i**3 for i in x]\n",
    "x4 = [i**4 for i in x]\n",
    "x5 = [i**5 for i in x]\n",
    "x_list = [x5,x4,x3,x2,x,np.ones_like(x)]\n",
    "# xhats5 = np.vstack((x5,x4,x3,x2,x,np.ones_like(x)))\n",
    "thetas = [[],[],[],[],[]]\n",
    "for i in range(N):\n",
    "    xhats = np.vstack(x_list[-(i+2):])\n",
    "#     print(xhats.shape)\n",
    "    xhats_T = xhats.T\n",
    "    x_inv = np.linalg.pinv(xhats.dot(xhats_T))\n",
    "    thetas[i] = x_inv.dot(xhats).dot(y)\n",
    "pass\n",
    "\n",
    "# ================== #\n",
    "# END YOUR CODE HERE #\n",
    "# ================== #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(thetas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the data\n",
    "f = plt.figure()\n",
    "ax = f.gca()\n",
    "ax.plot(x, y, '.')\n",
    "ax.set_xlabel('$x$')\n",
    "ax.set_ylabel('$y$')\n",
    "\n",
    "# Plot the regression lines\n",
    "plot_xs = []\n",
    "for i in np.arange(N):\n",
    "    if i == 0:\n",
    "        plot_x = np.vstack((np.linspace(min(x), max(x),50), np.ones(50)))\n",
    "    else:\n",
    "        plot_x = np.vstack((plot_x[-2]**(i+1), plot_x))                              \n",
    "    plot_xs.append(plot_x)\n",
    "\n",
    "for i in np.arange(N):\n",
    "    ax.plot(plot_xs[i][-2,:], thetas[i].dot(plot_xs[i]))\n",
    "\n",
    "labels = ['data']\n",
    "[labels.append('n={}'.format(i+1)) for i in np.arange(N)]\n",
    "bbox_to_anchor=(1.3, 1)\n",
    "lgd = ax.legend(labels, bbox_to_anchor=bbox_to_anchor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating the training error (5 points)\n",
    "\n",
    "Here, we'll now calculate the training error of polynomial models of orders 1 to 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_errors = []\n",
    "\n",
    "# ==================== #\n",
    "# START YOUR CODE HERE #\n",
    "# ==================== #\n",
    "\n",
    "# GOAL: create a variable training_errors, a list of 5 elements,\n",
    "# where training_errors[i] are the training loss for the polynomial fit of order i+1.\n",
    "for i in range(N):\n",
    "    theta = thetas[i]\n",
    "    xhats = np.vstack(x_list[-(i+2):])\n",
    "    y_pred = theta.dot(xhats)\n",
    "    train_err = (np.sum(np.square(np.array(y - y_pred))) / num_train) ** 0.5\n",
    "    training_errors.append(train_err)\n",
    "pass\n",
    "\n",
    "# ================== #\n",
    "# END YOUR CODE HERE #\n",
    "# ================== #\n",
    "\n",
    "print ('Training errors are: \\n', training_errors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QUESTIONS\n",
    "\n",
    "(1) What polynomial has the best training error?\n",
    "\n",
    "(2) Why is this expected?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ANSWERS\n",
    "\n",
    "(1) I think the 3/4/5 has the best training error (though not exactly same but very similar)\n",
    "\n",
    "(2) This is because the generating model has a polynomial order of 3, so the linear model or the polynomial model of order 2 couldn't fit the generating model the best.\n",
    "As for models with higher polynomial order, they are more complicated and the coefficients for x^4 or x^5 will be automatically set to zero according to our derivative, so 3/4/5 polynomial models have better and similar performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating new samples and testing error (5 points)\n",
    "\n",
    "Here, we'll now generate new samples and calculate testing error of polynomial models of orders 1 to 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.random.uniform(low=1, high=2, size=(num_train,))\n",
    "y = x - 2*x**2 + x**3 + np.random.normal(loc=0, scale=0.03, size=(num_train,))\n",
    "f = plt.figure()\n",
    "ax = f.gca()\n",
    "ax.plot(x, y, '.')\n",
    "ax.set_xlabel('$x$')\n",
    "ax.set_ylabel('$y$')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xhats = []\n",
    "for i in np.arange(N):\n",
    "    if i == 0:\n",
    "        xhat = np.vstack((x, np.ones_like(x)))\n",
    "        plot_x = np.vstack((np.linspace(min(x), max(x),50), np.ones(50)))\n",
    "    else:\n",
    "        xhat = np.vstack((x**(i+1), xhat))\n",
    "        plot_x = np.vstack((plot_x[-2]**(i+1), plot_x))\n",
    "                              \n",
    "    xhats.append(xhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the data\n",
    "f = plt.figure()\n",
    "ax = f.gca()\n",
    "ax.plot(x, y, '.')\n",
    "ax.set_xlabel('$x$')\n",
    "ax.set_ylabel('$y$')\n",
    "\n",
    "# Plot the regression lines\n",
    "plot_xs = []\n",
    "for i in np.arange(N):\n",
    "    if i == 0:\n",
    "        plot_x = np.vstack((np.linspace(min(x), max(x),50), np.ones(50)))\n",
    "    else:\n",
    "        plot_x = np.vstack((plot_x[-2]**(i+1), plot_x))                              \n",
    "    plot_xs.append(plot_x)\n",
    "\n",
    "for i in np.arange(N):\n",
    "    ax.plot(plot_xs[i][-2,:], thetas[i].dot(plot_xs[i]))\n",
    "\n",
    "labels = ['data']\n",
    "[labels.append('n={}'.format(i+1)) for i in np.arange(N)]\n",
    "bbox_to_anchor=(1.3, 1)\n",
    "lgd = ax.legend(labels, bbox_to_anchor=bbox_to_anchor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_errors = []\n",
    "\n",
    "# ==================== #\n",
    "# START YOUR CODE HERE #\n",
    "# ==================== #\n",
    "\n",
    "# GOAL: create a variable testing_errors, a list of 5 elements,\n",
    "# where testing_errors[i] are the testing loss for the polynomial fit of order i+1.\n",
    "for i in range(N):\n",
    "    theta = thetas[i]\n",
    "    x = xhats[i]\n",
    "#     print(x.shape)\n",
    "    y_pred = theta.dot(x)\n",
    "    test_err = (np.sum(np.square(np.array(y - y_pred))) / num_train) ** 0.5\n",
    "    testing_errors.append(test_err)\n",
    "pass\n",
    "\n",
    "# ================== #\n",
    "# END YOUR CODE HERE #\n",
    "# ================== #\n",
    "\n",
    "print ('Testing errors are: \\n', testing_errors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QUESTIONS\n",
    "\n",
    "(1) What polynomial has the best testing error?\n",
    "\n",
    "(2) Why polynomial models of orders 5 does not generalize well?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ANSWERS\n",
    "\n",
    "(1) The 4 polynoimal ths the best testing error: nearly 0.109.\n",
    "\n",
    "(2) Because our data is actually not complex, and the polynomial of 5 is too complicated for such a simple dataset, which will cause overfitting, which means it may fit the training data pretty well but would might have a bad performance when dealing with new data (testing data)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "219proj1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "678dded93ceebc1b80a429ea0b939707448da00f422398de3c7e5a157a0025e9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
