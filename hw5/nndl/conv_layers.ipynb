{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af549e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load conv_layers.py\n",
    "import numpy as np\n",
    "from nndl.layers import *\n",
    "import pdb\n",
    "\n",
    "\n",
    "def conv_forward_naive(x, w, b, conv_param):\n",
    "  \"\"\"\n",
    "  A naive implementation of the forward pass for a convolutional layer.\n",
    "\n",
    "  The input consists of N data points, each with C channels, height H and width\n",
    "  W. We convolve each input with F different filters, where each filter spans\n",
    "  all C channels and has height HH and width HH.\n",
    "\n",
    "  Input:\n",
    "  - x: Input data of shape (N, C, H, W)\n",
    "  - w: Filter weights of shape (F, C, HH, WW)\n",
    "  - b: Biases, of shape (F,)\n",
    "  - conv_param: A dictionary with the following keys:\n",
    "    - 'stride': The number of pixels between adjacent receptive fields in the\n",
    "      horizontal and vertical directions.\n",
    "    - 'pad': The number of pixels that will be used to zero-pad the input.\n",
    "\n",
    "  Returns a tuple of:\n",
    "  - out: Output data, of shape (N, F, H', W') where H' and W' are given by\n",
    "    H' = 1 + (H + 2 * pad - HH) / stride\n",
    "    W' = 1 + (W + 2 * pad - WW) / stride\n",
    "  - cache: (x, w, b, conv_param)\n",
    "  \"\"\"\n",
    "  out = None\n",
    "  pad = conv_param['pad']\n",
    "  stride = conv_param['stride']\n",
    "\n",
    "  # ================================================================ #\n",
    "  # YOUR CODE HERE:\n",
    "  #   Implement the forward pass of a convolutional neural network.\n",
    "  #   Store the output as 'out'.\n",
    "  #   Hint: to pad the array, you can use the function np.pad.\n",
    "  # ================================================================ #\n",
    "  pad_size = ((0,0), (0,0), (pad, pad), (pad, pad))\n",
    "  pad_x = np.pad(x, pad_size, mode = 'constant', constant_values=0)\n",
    "  N, C, H, W = x.shape\n",
    "  F, C, HH, WW = w.shape\n",
    "  output_H = int(1 + (H + 2*pad - HH) / stride)\n",
    "  output_W = int(1 + (W + 2*pad - WW) / stride)\n",
    "  \n",
    "  out = np.zeros((N, F, output_H, output_W))\n",
    "  for n in range(N):\n",
    "    for h in range(output_H):\n",
    "      for ww in range(output_W):\n",
    "        # get the (k, 1, h*stride : h*stride+HH, w*stride : w*stride+WW) part\n",
    "        x__conv_part = pad_x[n, :, h*stride:h*stride+HH, ww*stride:ww*stride+WW]\n",
    "        x_after_conv_part = x__conv_part * w\n",
    "        out[n, :, h, ww] = np.sum(x_after_conv_part, axis = (1,2,3))\n",
    "  out = out + b[None, :, None, None]\n",
    "  pass\n",
    "\n",
    "  # ================================================================ #\n",
    "  # END YOUR CODE HERE\n",
    "  # ================================================================ #\n",
    "    \n",
    "  cache = (x, w, b, conv_param)\n",
    "  return out, cache\n",
    "\n",
    "\n",
    "def conv_backward_naive(dout, cache):\n",
    "  \"\"\"\n",
    "  A naive implementation of the backward pass for a convolutional layer.\n",
    "\n",
    "  Inputs:\n",
    "  - dout: Upstream derivatives.\n",
    "  - cache: A tuple of (x, w, b, conv_param) as in conv_forward_naive\n",
    "\n",
    "  Returns a tuple of:\n",
    "  - dx: Gradient with respect to x\n",
    "  - dw: Gradient with respect to w\n",
    "  - db: Gradient with respect to b\n",
    "  \"\"\"\n",
    "  dx, dw, db = None, None, None\n",
    "\n",
    "  N, F, out_height, out_width = dout.shape\n",
    "  x, w, b, conv_param = cache\n",
    "  \n",
    "  stride, pad = [conv_param['stride'], conv_param['pad']]\n",
    "  xpad = np.pad(x, ((0,0), (0,0), (pad,pad), (pad,pad)), mode='constant')\n",
    "  num_filts, _, f_height, f_width = w.shape\n",
    "\n",
    "  # ================================================================ #\n",
    "  # YOUR CODE HERE:\n",
    "  #   Implement the backward pass of a convolutional neural network.\n",
    "  #   Calculate the gradients: dx, dw, and db.\n",
    "  # ================================================================ #\n",
    "  db = np.sum(dout, axis = (0,2,3))\n",
    "  dw = np.zeros(w.shape)\n",
    "  dx_pad = np.zeros(xpad.shape)\n",
    "  for n in range(N):\n",
    "    for hh in range(out_height):\n",
    "      for ww in range(out_width):\n",
    "        dx_pad[n, :, hh*stride:hh*stride + f_height, ww*stride:ww*stride + f_width] += np.sum((w*(dout[n, :, hh, ww])[:,None ,None, None]), axis=0)\n",
    "        dw += xpad[n,:,hh*stride:hh*stride+f_height,ww*stride:ww*stride+f_width]*(dout[n,:,hh,ww])[:,None,None,None]\n",
    "  dx = dx_pad[:,:,pad:-pad,pad:-pad]\n",
    "  pass\n",
    "\n",
    "  # ================================================================ #\n",
    "  # END YOUR CODE HERE\n",
    "  # ================================================================ #\n",
    "\n",
    "  return dx, dw, db\n",
    "\n",
    "\n",
    "def max_pool_forward_naive(x, pool_param):\n",
    "  \"\"\"\n",
    "  A naive implementation of the forward pass for a max pooling layer.\n",
    "\n",
    "  Inputs:\n",
    "  - x: Input data, of shape (N, C, H, W)\n",
    "  - pool_param: dictionary with the following keys:\n",
    "    - 'pool_height': The height of each pooling region\n",
    "    - 'pool_width': The width of each pooling region\n",
    "    - 'stride': The distance between adjacent pooling regions\n",
    "\n",
    "  Returns a tuple of:\n",
    "  - out: Output data\n",
    "  - cache: (x, pool_param)\n",
    "  \"\"\"\n",
    "  out = None\n",
    "  \n",
    "  # ================================================================ #\n",
    "  # YOUR CODE HERE:\n",
    "  #   Implement the max pooling forward pass.\n",
    "  # ================================================================ #\n",
    "  N,C,H,W = x.shape\n",
    "  HH, WW, S = pool_param['pool_height'], pool_param['pool_width'], pool_param['stride']\n",
    "  out_H = int(1 + (H - HH) / S)\n",
    "  out_W = int(1 + (W - WW) / S)\n",
    "  out = np.zeros((N,C,out_H,out_W))\n",
    "\n",
    "  for i in range(out_H):\n",
    "    for j in range(out_W):\n",
    "      x_pooling_part = x[:,:,i*S : i*S+HH, j*S : j*S+WW]\n",
    "      out[:, :, i, j] = np.max(x_pooling_part, axis = (2,3))\n",
    "  pass\n",
    "  # ================================================================ #\n",
    "  # END YOUR CODE HERE\n",
    "  # ================================================================ # \n",
    "  cache = (x, pool_param)\n",
    "  return out, cache\n",
    "\n",
    "def max_pool_backward_naive(dout, cache):\n",
    "  \"\"\"\n",
    "  A naive implementation of the backward pass for a max pooling layer.\n",
    "\n",
    "  Inputs:\n",
    "  - dout: Upstream derivatives\n",
    "  - cache: A tuple of (x, pool_param) as in the forward pass.\n",
    "\n",
    "  Returns:\n",
    "  - dx: Gradient with respect to x\n",
    "  \"\"\"\n",
    "  dx = None\n",
    "  x, pool_param = cache\n",
    "  pool_height, pool_width, stride = pool_param['pool_height'], pool_param['pool_width'], pool_param['stride']\n",
    "\n",
    "  # ================================================================ #\n",
    "  # YOUR CODE HERE:\n",
    "  #   Implement the max pooling backward pass.\n",
    "  # ================================================================ #\n",
    "  H,W = x.shape[2],x.shape[3]\n",
    "  HH,WW,S = pool_height, pool_width, stride\n",
    "  out_H = int(1 + (H - HH) / S)\n",
    "  out_W = int(1 + (W - WW) / S)\n",
    "  dx = np.zeros_like(x)\n",
    "  for i in range(out_H):\n",
    "    for j in range(out_W):\n",
    "      x_pooling_part = x[:,:,i*stride : i*S+HH,j*S : j*S+WW]\n",
    "      pooling_idx = np.max(x_pooling_part, axis = (2,3))\n",
    "      pooling_mask = np.zeros_like(x_pooling_part)\n",
    "      pooling_mask = pooling_idx[:,:,None,None]==x_pooling_part\n",
    "      dx[:,:,i*S : i*S+HH,j*S : j*S+WW] += pooling_mask*(dout[:,:,i,j])[:,:,None,None]\n",
    "  pass\n",
    "\n",
    "  # ================================================================ #\n",
    "  # END YOUR CODE HERE\n",
    "  # ================================================================ # \n",
    "\n",
    "  return dx\n",
    "\n",
    "def spatial_batchnorm_forward(x, gamma, beta, bn_param):\n",
    "  \"\"\"\n",
    "  Computes the forward pass for spatial batch normalization.\n",
    "  \n",
    "  Inputs:\n",
    "  - x: Input data of shape (N, C, H, W)\n",
    "  - gamma: Scale parameter, of shape (C,)\n",
    "  - beta: Shift parameter, of shape (C,)\n",
    "  - bn_param: Dictionary with the following keys:\n",
    "    - mode: 'train' or 'test'; required\n",
    "    - eps: Constant for numeric stability\n",
    "    - momentum: Constant for running mean / variance. momentum=0 means that\n",
    "      old information is discarded completely at every time step, while\n",
    "      momentum=1 means that new information is never incorporated. The\n",
    "      default of momentum=0.9 should work well in most situations.\n",
    "    - running_mean: Array of shape (D,) giving running mean of features\n",
    "    - running_var Array of shape (D,) giving running variance of features\n",
    "    \n",
    "  Returns a tuple of:\n",
    "  - out: Output data, of shape (N, C, H, W)\n",
    "  - cache: Values needed for the backward pass\n",
    "  \"\"\"\n",
    "  out, cache = None, None\n",
    "\n",
    "  # ================================================================ #\n",
    "  # YOUR CODE HERE:\n",
    "  #   Implement the spatial batchnorm forward pass.\n",
    "  #\n",
    "  #   You may find it useful to use the batchnorm forward pass you \n",
    "  #   implemented in HW #4.\n",
    "  # ================================================================ #\n",
    "  N,C,H,W = x.shape\n",
    "#   out = np.zeros((N,C,H,W))\n",
    "  #reshape X into shape: (N*H*W, C) so that we can do normalization on layers\n",
    "  x_reshape = x.transpose((0,2,3,1)).reshape(N*H*W, C)\n",
    "  out, cache = batchnorm_forward(x_reshape, gamma, beta, bn_param)\n",
    "  out = out.reshape(N, H, W, C).transpose((0, 3, 1, 2))\n",
    "  pass\n",
    "\n",
    "  # ================================================================ #\n",
    "  # END YOUR CODE HERE\n",
    "  # ================================================================ # \n",
    "\n",
    "  return out, cache\n",
    "\n",
    "\n",
    "def spatial_batchnorm_backward(dout, cache):\n",
    "  \"\"\"\n",
    "  Computes the backward pass for spatial batch normalization.\n",
    "  \n",
    "  Inputs:\n",
    "  - dout: Upstream derivatives, of shape (N, C, H, W)\n",
    "  - cache: Values from the forward pass\n",
    "  \n",
    "  Returns a tuple of:\n",
    "  - dx: Gradient with respect to inputs, of shape (N, C, H, W)\n",
    "  - dgamma: Gradient with respect to scale parameter, of shape (C,)\n",
    "  - dbeta: Gradient with respect to shift parameter, of shape (C,)\n",
    "  \"\"\"\n",
    "  dx, dgamma, dbeta = None, None, None\n",
    "\n",
    "  # ================================================================ #\n",
    "  # YOUR CODE HERE:\n",
    "  #   Implement the spatial batchnorm backward pass.\n",
    "  #\n",
    "  #   You may find it useful to use the batchnorm forward pass you \n",
    "  #   implemented in HW #4.\n",
    "  # ================================================================ #\n",
    "  N,C,H,W = dout.shape\n",
    "  dout = dout.transpose((0, 2, 3, 1)).reshape(N * H * W, C)\n",
    "  dx, dgamma, dbeta = batchnorm_backward(dout, cache)\n",
    "  dx = dx.reshape(N, H, W, C).transpose((0, 3, 1, 2))\n",
    "  # ================================================================ #\n",
    "  # END YOUR CODE HERE\n",
    "  # ================================================================ # \n",
    "\n",
    "  return dx, dgamma, dbeta"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
