{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6155e8ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load layers.py\n",
    "import numpy as np\n",
    "import pdb\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def affine_forward(x, w, b):\n",
    "  \"\"\"\n",
    "  Computes the forward pass for an affine (fully-connected) layer.\n",
    "\n",
    "  The input x has shape (N, d_1, ..., d_k) and contains a minibatch of N\n",
    "  examples, where each example x[i] has shape (d_1, ..., d_k). We will\n",
    "  reshape each input into a vector of dimension D = d_1 * ... * d_k, and\n",
    "  then transform it to an output vector of dimension M.\n",
    "\n",
    "  Inputs:\n",
    "  - x: A numpy array containing input data, of shape (N, d_1, ..., d_k)\n",
    "  - w: A numpy array of weights, of shape (D, M)\n",
    "  - b: A numpy array of biases, of shape (M,)\n",
    "  \n",
    "  Returns a tuple of:\n",
    "  - out: output, of shape (N, M)\n",
    "  - cache: (x, w, b)\n",
    "  \"\"\"\n",
    "\n",
    "  # ================================================================ #\n",
    "  # YOUR CODE HERE:\n",
    "  #   Calculate the output of the forward pass.  Notice the dimensions\n",
    "  #   of w are D x M, which is the transpose of what we did in earlier \n",
    "  #   assignments.\n",
    "  # ================================================================ #\n",
    "  num_inputs = x.shape[0]\n",
    "  input_shape = x.shape[1:]\n",
    "  input_size = np.prod(input_shape)\n",
    "  x_reshape = x.reshape(num_inputs, input_size)\n",
    "  out = np.dot(x_reshape, w) + b\n",
    "  pass\n",
    "\n",
    "  # ================================================================ #\n",
    "  # END YOUR CODE HERE\n",
    "  # ================================================================ #\n",
    "    \n",
    "  cache = (x, w, b)\n",
    "  return out, cache\n",
    "\n",
    "\n",
    "def affine_backward(dout, cache):\n",
    "  \"\"\"\n",
    "  Computes the backward pass for an affine layer.\n",
    "\n",
    "  Inputs:\n",
    "  - dout: Upstream derivative, of shape (N, M)\n",
    "  - cache: Tuple of:\n",
    "    - x: Input data, of shape (N, d_1, ... d_k)\n",
    "    - w: Weights, of shape (D, M)\n",
    "\n",
    "  Returns a tuple of:\n",
    "  - dx: Gradient with respect to x, of shape (N, d1, ..., d_k)\n",
    "  - dw: Gradient with respect to w, of shape (D, M)\n",
    "  - db: Gradient with respect to b, of shape (M,)\n",
    "  \"\"\"\n",
    "  x, w, b = cache\n",
    "  dx, dw, db = None, None, None\n",
    "\n",
    "  # ================================================================ #\n",
    "  # YOUR CODE HERE:\n",
    "  #   Calculate the gradients for the backward pass.\n",
    "  # ================================================================ #\n",
    "\n",
    "  # dout is N x M\n",
    "  # dx should be N x d1 x ... x dk; it relates to dout through multiplication with w, which is D x M\n",
    "  # dw should be D x M; it relates to dout through multiplication with x, which is N x D after reshaping\n",
    "  # db should be M; it is just the sum over dout examples\n",
    "  num_inputs = x.shape[0]\n",
    "  input_shape = x.shape[1:]\n",
    "  input_size = np.prod(input_shape)\n",
    "  x_reshape = x.reshape(num_inputs, input_size)\n",
    "  x_shape = x.shape\n",
    "  db = np.sum(dout, axis = 0)\n",
    "  dx = np.dot(dout, w.T).reshape(x_shape)\n",
    "  dw = np.dot(x_reshape.T,dout)\n",
    "  pass\n",
    "\n",
    "  # ================================================================ #\n",
    "  # END YOUR CODE HERE\n",
    "  # ================================================================ #\n",
    "  \n",
    "  return dx, dw, db\n",
    "\n",
    "def relu_forward(x):\n",
    "  \"\"\"\n",
    "  Computes the forward pass for a layer of rectified linear units (ReLUs).\n",
    "\n",
    "  Input:\n",
    "  - x: Inputs, of any shape\n",
    "\n",
    "  Returns a tuple of:\n",
    "  - out: Output, of the same shape as x\n",
    "  - cache: x\n",
    "  \"\"\"\n",
    "  # ================================================================ #\n",
    "  # YOUR CODE HERE:\n",
    "  #   Implement the ReLU forward pass.\n",
    "  # ================================================================ #\n",
    "  out = np.maximum(x,0)\n",
    "  pass\n",
    "  # ================================================================ #\n",
    "  # END YOUR CODE HERE\n",
    "  # ================================================================ #\n",
    " \n",
    "  cache = x\n",
    "  return out, cache\n",
    "\n",
    "\n",
    "def relu_backward(dout, cache):\n",
    "  \"\"\"\n",
    "  Computes the backward pass for a layer of rectified linear units (ReLUs).\n",
    "\n",
    "  Input:\n",
    "  - dout: Upstream derivatives, of any shape\n",
    "  - cache: Input x, of same shape as dout\n",
    "\n",
    "  Returns:\n",
    "  - dx: Gradient with respect to x\n",
    "  \"\"\"\n",
    "  x = cache\n",
    "\n",
    "  # ================================================================ #\n",
    "  # YOUR CODE HERE:\n",
    "  #   Implement the ReLU backward pass\n",
    "  # ================================================================ #\n",
    "  x[x<0] = 0\n",
    "  x[x>0] = 1\n",
    "  dx = np.multiply(x,dout)\n",
    "  # ReLU directs linearly to those > 0\n",
    "  pass\n",
    "    \n",
    "  # ================================================================ #\n",
    "  # END YOUR CODE HERE\n",
    "  # ================================================================ #\n",
    " \n",
    "  return dx\n",
    "\n",
    "\n",
    "def softmax_loss(x, y):\n",
    "  \"\"\"\n",
    "  Computes the loss and gradient for softmax classification.\n",
    "\n",
    "  Inputs:\n",
    "  - x: Input data, of shape (N, C) where x[i, j] is the score for the jth class\n",
    "    for the ith input.\n",
    "  - y: Vector of labels, of shape (N,) where y[i] is the label for x[i] and\n",
    "    0 <= y[i] < C\n",
    "\n",
    "  Returns a tuple of:\n",
    "  - loss: Scalar giving the loss\n",
    "  - dx: Gradient of the loss with respect to x\n",
    "  \"\"\"\n",
    "\n",
    "  probs = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "  probs /= np.sum(probs, axis=1, keepdims=True)\n",
    "  N = x.shape[0]\n",
    "  loss = -np.sum(np.log(probs[np.arange(N), y])) / N\n",
    "  dx = probs.copy()\n",
    "  dx[np.arange(N), y] -= 1\n",
    "  dx /= N\n",
    "  return loss, dx\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
